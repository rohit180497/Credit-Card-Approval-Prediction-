---
title: "Draft Model Credit Card Approval Prediction"
output:
  word_document: default
  html_notebook: default
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

```{r}
### Importing important libraries
library(dplyr)
library(tidyverse)
library(ggplot2)
library(leaflet)
library(scales)
library(caret)
library(corrplot)
library(smotefamily)
library(performanceEstimation)
library(ROSE)
library(randomForest)
library(glmnet)
library(pROC)
library(e1071)
```


```{r}
# Loading data
application <- read.csv("application_record.csv")
credit<- read.csv("credit_record.csv")
```


```{r}
# str(application)
# str(credit)
glimpse(credit)
```

### Target setting

STATUS: 0: 1-29 days past due 1: 30-59 days past due 2: 60-89 days overdue 3: 90-119 days overdue 4: 120-149 days overdue 5: Overdue or bad debts, write-offs for more than 150 days C: paid off that month X: No loan for the month.

Customer with over 30 days or more past due would be considered as a ‘bad’ customer. 
Based on this consideration, we can adjust the value of “STATUS” column to be 1 (bad customer) and 0 (good customer) and make it as our target for our model.
```{r}

credit$target <- credit$STATUS
unique(credit$target)

credit$target <- ifelse(credit$target == 'X' | credit$target == 'C', 0, credit$target)
credit$target <- as.integer(credit$target) 
credit$target[credit$target >= 1] <- 1 
credit_df <- aggregate(target ~ ID, data = credit, FUN = max)
```
Target Rate 

```{r}
dplyr::count(credit_df, target, sort = TRUE)
```
Cleaning Application data
```{r}
# Changing the data type and adjusting some character values to binary
application <- application %>% 
  mutate_if(is.character, as.factor) %>% 
  mutate_if(is.double, as.integer) %>% 
  mutate(CODE_GENDER = as.integer(ifelse(CODE_GENDER == "M", 1, 0)),
         FLAG_OWN_CAR = as.integer(ifelse(FLAG_OWN_CAR == "Y", 1, 0)),
         FLAG_OWN_REALTY = as.integer(ifelse(FLAG_OWN_REALTY == "Y", 1, 0)))
```



```{r}
#Merging credit_record and application_record
credit_data <- merge(application, credit_df, by = "ID") %>% 
  select(-c(ID))
head(credit_data)
```


```{r}
# target lables

count_data <- dplyr::count(credit_data, target, sort = TRUE)

count_data$target<- factor(count_data$target, levels = c("0", "1"),
                                     labels = c("Good Customer", "Bad Customer"))

count_data$percentage <- (count_data$n / sum(count_data$n)) * 100

# Plotting the barplot with different colors for each bar
ggplot(count_data, aes(x = as.factor(target), y = n, fill = target)) +
  geom_bar(stat = "identity", color = "black") +
  geom_text(aes(label = paste0(round(percentage), "%")), vjust = -0.5, color = "black", size = 4) +  # Adding percentage labels
scale_fill_manual(values = c("skyblue", "salmon"), breaks = levels(count_data$target_variable)) +  
  labs(#title = "Distribution of Target Variable",
       x = "Target",
       y = "Count") +
  theme_minimal()
```


```{r}
dplyr::count(credit_data, target, sort = TRUE)
```
### Exploratory Data Analysis

```{r}
names(credit_data)
```


```{r}
credit_data %>%
  dplyr::group_by(NAME_INCOME_TYPE, target) %>%
  dplyr::summarise(Freq = n(), .groups = 'drop') %>%
  tidyr::pivot_wider(names_from = target, values_from = Freq, values_fill = 0) %>%
  dplyr::arrange(NAME_INCOME_TYPE)

```


```{r}
# Contribution of Education type in target

credit_data %>%
  dplyr::group_by(NAME_EDUCATION_TYPE, target) %>%
  dplyr::summarise(Freq = n(), .groups = 'drop') %>%
  tidyr::pivot_wider(names_from = target, values_from = Freq, values_fill = 0) %>%
  dplyr::arrange(NAME_EDUCATION_TYPE)
```


```{r}
# Contribution of Family status in target

credit_data %>%
  dplyr::group_by(NAME_FAMILY_STATUS, target) %>%
  dplyr::summarise(Freq = n(), .groups = 'drop') %>%
  tidyr::pivot_wider(names_from = target, values_from = Freq, values_fill = 0) %>%
  dplyr::arrange(NAME_FAMILY_STATUS)
```


```{r}
# Contribution of Housing type in target

credit_data %>%
  dplyr::group_by(NAME_HOUSING_TYPE, target) %>%
  dplyr::summarise(Freq = n(), .groups = 'drop') %>%
  tidyr::pivot_wider(names_from = target, values_from = Freq, values_fill = 0) %>%
  dplyr::arrange(NAME_HOUSING_TYPE)
```


```{r}
# Contribution of Occupation type in target

credit_data %>%
  dplyr::group_by(OCCUPATION_TYPE, target) %>%
  dplyr::summarise(Freq = n(),.groups = 'drop' ) %>%
  tidyr::pivot_wider(names_from = target, values_from = Freq, values_fill = 0) %>%
  dplyr::arrange(OCCUPATION_TYPE)
```
### Data Preprocessing

```{r}
credit_data <- credit_data %>%
  mutate(GROUPED_INCOME_TYPE = case_when(
    NAME_INCOME_TYPE %in% c("Commercial associate", "State servant","Working") ~ "Employed",
    NAME_INCOME_TYPE %in% c("Pensioner") ~ "Retired",
    TRUE ~ "Other"
  ))
unique(credit_data$GROUPED_INCOME_TYPE)
```


```{r}
credit_data <- credit_data %>%
  mutate(GROUPED_EDUCATION_TYPE = case_when(
    NAME_EDUCATION_TYPE %in% c("Academic degree", "Higher education") ~ "Higher education",
    NAME_EDUCATION_TYPE %in% c("Incomplete higher","Secondary / secondary special") ~"Secondary Special",
    TRUE ~ "Other"
  ))
unique(credit_data$GROUPED_EDUCATION_TYPE)

```


```{r}
credit_data <- credit_data %>%
  mutate(GROUPED_FAMILY_STATUS = case_when(
    NAME_FAMILY_STATUS %in% c("Civil marriage", "Married") ~ "Married",
    NAME_FAMILY_STATUS %in% c("Separated", "Single / not married") ~ "Single",
    TRUE ~ "Other"
  ))
unique(credit_data$GROUPED_FAMILY_STATUS)
```


```{r}
credit_data <- credit_data %>%
  mutate(NAME_HOUSING_TYPE = case_when(
    NAME_HOUSING_TYPE %in% c("Co-op apartment", "House / apartment", "With parents") ~ "House Apt",
    NAME_HOUSING_TYPE %in% c("Municipal apartment", "Office apartment") ~ "Office Apt",
    TRUE ~ "Other"
  ))
unique(credit_data$NAME_HOUSING_TYPE)
```


```{r}
credit_data <- credit_data %>%
  mutate(GROUPED_OCCUPATION_TYPE = case_when(
    OCCUPATION_TYPE %in% c("Accountants", "High skill tech staff", "IT staff", "Managers") ~ "Professional",
    OCCUPATION_TYPE %in% c("Cleaning staff", "Cooking staff", "Drivers", "Laborers", "Low-skill Laborers", "Medicine staff", "Private service staff", "Realty agents", "Sales staff", "Security staff", "Waiters/barmen staff") ~ "Service/Manual",
    OCCUPATION_TYPE %in% c("Core staff", "Secretaries") ~ "Support",
    TRUE ~ "Other"
  )) 
unique(credit_data$GROUPED_OCCUPATION_TYPE)
```


```{r}
## Let's calculate age and Years employed for customer
credit_data$AGE <- round(-credit_data$DAYS_BIRTH/365.24, 0)
credit_data$YEARS_EMPLOYED <- round(-credit_data$DAYS_EMPLOYED/365.24, 0)
```


```{r}
negative_years_pensioner <- credit_data$YEARS_EMPLOYED < 0 & credit_data$NAME_INCOME_TYPE == "Pensioner"
credit_data$YEARS_EMPLOYED[negative_years_pensioner] <- 0
credit_data$YEARS_EMPLOYED[negative_years_pensioner] <- credit_data$AGE[negative_years_pensioner] - 18  # Assuming 18 as the starting age for employment

```


```{r}
## Let's subset credit data

final_df <- subset(credit_data, select = -c(NAME_INCOME_TYPE, NAME_EDUCATION_TYPE, NAME_FAMILY_STATUS, NAME_HOUSING_TYPE,OCCUPATION_TYPE,CNT_CHILDREN,
                                            DAYS_BIRTH, DAYS_EMPLOYED,FLAG_MOBIL))
str(final_df)
prop.table(table(final_df$target))
```

```{r}
## Standardizing annual income
mean_val <- mean(final_df$AMT_INCOME_TOTAL)
sd_val <- sd(final_df$AMT_INCOME_TOTAL)

# Apply Standardization
final_df$AMT_INCOME_TOTAL <- (final_df$AMT_INCOME_TOTAL - mean_val) / sd_val
```

### Train Test Split
```{r}
set.seed(80307)
train_indices <- sample(x= nrow(final_df), size= nrow(final_df)* 0.70)

# Split the data into training and test sets
train_data <- final_df[train_indices,]
test_data <- final_df[-train_indices,]

# Check the dimensions of the training and test sets
dim(train_data)
dim(test_data)

table(train_data$target)
table(test_data$target)
```

```{r}
# Prepare the data for modelling
train_x <- model.matrix(target ~ ., train_data)[, -1] 
test_x <- model.matrix(target ~ ., test_data)[,-1]

train_y <- train_data$target
test_y <- test_data$target

dim(train_x)
dim(test_x)
```

### Base Logistic Regression Model

```{r}
model_glm <- glm(target ~., data=train_data, family= binomial(link="logit"))
summary(model_glm)
```

```{r}
probabilities.train_glm <- predict(model_glm, newdata=train_data, type="response")
predicted.classes.train_glm <- ifelse(probabilities.train_glm > 0.5, 1, 0)

probabilities.train_glm = as.integer(probabilities.train_glm)
```


```{r}
# Model accuracy
#conf_matrix_glm <- confusionMatrix(probabilities.train_glm, train_data$target)
confusion_matrix_glm <- table(predicted.classes.train_glm, train_data$target)

```


```{r}
## Giving error
accuracy_glm <- sum(diag(confusion_matrix_glm)) / sum(confusion_matrix_glm)

unique(predicted.classes.train_glm)
```
Here we can see model is not able to predict 1 class and giving all 0 prediction

```{r}
# Let's try to take feature which have p-value>0.05 coefficients
significant_features <- c("CODE_GENDER", "FLAG_OWN_CAR", "FLAG_OWN_REALTY", "AMT_INCOME_TOTAL", "FLAG_EMAIL", "GROUPED_INCOME_TYPE", "GROUPED_OCCUPATION_TYPE", "AGE", "target")

selected_model <- glm(target ~ ., family = binomial(link = "logit"), data = train_data[, significant_features])

# View the summary of the new model
summary(selected_model)
```


```{r}
probabilities.train_glm2 <- predict(selected_model, newdata=train_data, type="response")
predicted.classes.train_glm2 <- ifelse(probabilities.train_glm2 > 0.5, 1, 0)

unique(predicted.classes.train_glm2)
accuracy_glm2 <- mean(predicted.classes.train_glm2 == train_y)


# Model accuracy
# conf_matrix_glm <- confusionMatrix(probabilities.train_glm, train_data$target, positive = "Yes")
## Giving error
```


```{r}
unique(predicted.classes.train_glm2)
# Giving only "no" in prediction
```
### Ridge Regularization

```{r}
# finding best value of lambda using cross validation 
set.seed(80307)
cv.ridge <- cv.glmnet(train_x, train_y, nfolds = 10, alpha=0)
plot(cv.ridge)
```


```{r}
cat("lambda.min:", cv.ridge$lambda.min, "\n")
cat("lambda.1se:", cv.ridge$lambda.1se, "\n")
```


```{r}
model.min_ridge <- glmnet(train_x, train_y, alpha= 0, lambda = cv.ridge$lambda.min)
model.min_ridge
# display the regression coefficient
coef(model.min_ridge)
summary(model.min_ridge)
```


```{r}
pred_probs <- predict(model.min_ridge, newx = test_x, s = "lambda.min", type = "response")
pred_class <- ifelse(pred_probs > 0.5, 1, 0)

conf_matrix <- table(Actual = test_y, Predicted = pred_class)
```


```{r}
pred_prob__ridge_train <- predict(model.min_ridge, newx = train_x, s = "lambda.min", type = "response")
pred_class__ridge_train <- ifelse(pred_prob__ridge_train > 0.5, 1, 0)

conf_matrix_ridge <- table(Actual = train_y, Predicted = pred_class__ridge_train)
conf_matrix_ridge

```


```{r}
accuracy_ridge <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Accuracy:", accuracy_ridge, "\n")
```
Unable to calculate recall because we don't have FP and TP 
Still after ridge regularization model is not able to predict class 1

### Lasso Regularization
```{r}
# finding best value of lambda using cross validation
set.seed(80307)
cv.lasso <- cv.glmnet(train_x, train_y, nfolds = 10, alpha=1)
plot(cv.lasso)
```


```{r}
cat("lambda.min_lasso:", cv.lasso$lambda.min, "\n")
cat("lambda.1se_lasso:", cv.lasso$lambda.1se, "\n")
```

```{r}
model.min_lasso <- glmnet(train_x, train_y, alpha= 1, lambda = cv.lasso$lambda.min)
model.min_lasso
# display the regression coefficient
coef(model.min_lasso)
```


```{r}
pred_probs <- predict(model.min_lasso, newx = test_x, s = "lambda.min", type = "response")
pred_class <- ifelse(pred_probs > 0.5, 1, 0)
conf_matrix_lasso <- table(Actual = test_y, Predicted = pred_class)
conf_matrix_lasso
```

```{r}
accuracy_lasso <- sum(diag(conf_matrix_lasso)) / sum(conf_matrix_lasso)
cat("Accuracy:", accuracy_lasso, "\n")
```

### Support Vector Machine model
```{r}
set.seed(80307)
svm_model <- svm(x = train_x, y = train_y, kernel = "radial", cost = 0.1)
predictions_svm <- predict(svm_model, newdata = train_x)
predicted.classes.svm <- ifelse(predictions_svm >=0.5,1,0)
```


```{r}
conf_matrix_svm <- table(Actual = train_y, Predicted = predicted.classes.svm)
accuracy_svm <- sum(diag(conf_matrix_svm)) / sum(conf_matrix_svm)
cat("Accuracy:", accuracy_svm, "\n")
```
```{r}
conf_matrix_svm
```
SVM model is also not able to predict class 1

### Sampling
```{r}
# First we will drop a good customers records from data to balance it with vulnerable customers
train_data$target <- as.integer(train_data$target)

df_0 <- train_data %>% 
  filter(target == 0) %>% 
  slice(-c(1:8000))

df_1 <- train_data %>% 
  filter(target == 1)
```


```{r}
print(dim(df_0))
print(dim(df_1))
```


```{r}
credit_train <- rbind(df_0, df_1)
dim(credit_train)
```

#### Applying Upsample technique on training dataset
```{r}

credit_train$target <- as.factor(credit_train$target)

# Apply up-sampling
ups_df <- upSample(x = credit_train %>% select(-target),
                   y = credit_train$target,
                   yname = "target")
table(ups_df$target)
```

#### Prepare data for modelling 
```{r}
train_x <- model.matrix(target ~ ., ups_df)[, -1]
train_y <- ups_df$target
dim(train_x)
dim(test_x)
```


```{r}
table(test_data$target)
table(ups_df$target)
```
We can see Now training target is upsample and it's no more imbalanced dataset
```{r}
plot_confusion_matrix <- function(confusion_matrix) {
  # Convert confusion matrix to data frame
  confusion_df <- as.data.frame(as.table(confusion_matrix))
  names(confusion_df) <- c("Predicted", "Actual", "Count")

  # Define labels
  confusion_df$Predicted <- ifelse(confusion_df$Predicted == 1, "Bad Customer", "Good Customer")
  confusion_df$Actual <- ifelse(confusion_df$Actual == 1, "Bad Customer", "Good Customer")

  confusion_df$Color <- ifelse(confusion_df$Predicted == confusion_df$Actual, "Correct Prediction", "Incorrect Prediction")

  ggplot(confusion_df, aes(x = Predicted, y = Actual)) +
    geom_tile(aes(fill = Color)) +
    geom_text(aes(label = Count), vjust = 1, color = "black", size= 10) +
    scale_fill_manual(values = c("skyblue", "lightgrey"), 
                      breaks = c("Correct Prediction", "Incorrect Prediction"),
                      labels = c("Correct Prediction", "Incorrect Prediction"),
                      guide = FALSE) +
    labs(title = "Confusion Matrix",
         x = "Predicted Values",
         y = "Actual Values") +
    theme_minimal() +
    theme(axis.text.x = element_text(size= 18),
          axis.title.x = element_text(margin = ggplot2::margin(t = 20)),
          text = element_text(size = 18))
}
```

### Logistic Regression 
```{r}
model <- glm(target ~., data=ups_df, family= binomial())
summary(model)
```


```{r}
probabilities.test <- predict(model, newdata=test_data, type="response")
predicted.classes.test <- ifelse(probabilities.test>=0.5,1,0)
test_y <- as.factor(as.character(test_y))
predicted.classes.test <- as.factor(predicted.classes.test)
conf_matrix_lr <- table(Actual = test_y, Predicted = predicted.classes.test)
conf_matrix_lr
```

```{r}
accuracy_lr <- sum(diag(conf_matrix_lr)) / sum(conf_matrix_lr)
cat("Accuracy:", accuracy_lr, "\n")

recall_lr <- conf_matrix_lr[2, 2] / sum(conf_matrix_lr[2, ])
cat("Recall:", recall_lr, "\n")

precision_lr <- conf_matrix_lr[2, 2] / sum(conf_matrix_lr[, 2])
cat("Precision:", precision_lr, "\n")

f1_score_lr <- 2 * (precision_lr * recall_lr) / (precision_lr + recall_lr)
cat("F1-Score:", f1_score_lr, "\n")

roc_curve_lr <- roc(test_y, as.integer(predicted.classes.test))
roc_auc_lr <- auc(roc_curve_lr)
cat("ROC-AUC Score:", roc_auc_lr, "\n")
```

### Ridge Regularization
```{r}
# finding best value of lambda using cross validation 
train_y = as.integer(as.character(train_y))
set.seed(80307)
cv.ridge_reg <- cv.glmnet(train_x, train_y, nfolds = 10, alpha=0)
plot(cv.ridge_reg)
```

```{r}
cat("lambda.min:", cv.ridge_reg$lambda.min, "\n")
cat("lambda.1se:", cv.ridge_reg$lambda.1se, "\n")
```


```{r}
model.min_ridge_reg <- glmnet(train_x, train_y, alpha= 0, lambda = cv.ridge_reg$lambda.min)

coef(model.min_ridge_reg)
summary(model.min_ridge_reg)
```


```{r}
pred_probs <- predict(model.min_ridge_reg, newx = test_x, s = "lambda.min", type = "response")
pred_class <- ifelse(pred_probs > 0.5, 1, 0)

conf_matrix_ridge_reg <- table(Actual = test_y, Predicted = pred_class)
conf_matrix_ridge_reg
```


```{r}
accuracy_ridge <- sum(diag(conf_matrix_ridge_reg)) / sum(conf_matrix_ridge_reg)
cat("Accuracy:", accuracy_ridge, "\n")

recall_ridge <- conf_matrix_ridge_reg[2, 2] / sum(conf_matrix_ridge_reg[2, ])
cat("Recall:", accuracy_ridge, "\n")

precision_ridge <- conf_matrix_lr[2, 2] / sum(conf_matrix_lr[, 2])
cat("Precision:", precision_ridge, "\n")

f1_score_ridge <- 2 * (precision_lr * recall_lr) / (precision_lr + recall_lr)
cat("F1-Score:", f1_score_ridge, "\n")

roc_curve_ridge <- roc(test_y, as.integer(predicted.classes.test))
roc_auc_ridge <- auc(roc_curve_ridge)
cat("ROC-AUC Score:", roc_auc_ridge, "\n")
```

### Lasso Regularization
```{r}
# finding best value of lambda using cross validation
set.seed(80307)
cv.lasso_reg <- cv.glmnet(train_x, train_y, nfolds = 10, alpha=1)
plot(cv.lasso_reg)

```
```{r}
cat("lambda.min_lasso:", cv.lasso_reg$lambda.min, "\n")
cat("lambda.1se_lasso:", cv.lasso_reg$lambda.1se, "\n")
```

```{r}
model.min_lasso_reg <- glmnet(train_x, train_y, alpha= 1, lambda = cv.lasso_reg$lambda.min)
model.min_lasso_reg
# display the regression coefficient
coef(model.min_lasso_reg)
```

```{r}
pred_probs <- predict(model.min_lasso_reg, newx = test_x, s = "lambda.min", type = "response")
pred_class <- ifelse(pred_probs > 0.5, 1, 0)

conf_matrix_lasso_reg <- table(Actual = test_y, Predicted = pred_class)
conf_matrix_lasso_reg
```

```{r}
accuracy_lasso <- sum(diag(conf_matrix_lasso_reg)) / sum(conf_matrix_lasso_reg)
cat("Accuracy:", accuracy_lasso, "\n")

recall_lasso <- conf_matrix_lasso_reg[2, 2] / sum(conf_matrix_lasso_reg[2, ])
cat("Recall:", recall_lasso, "\n")

precision_lasso <- conf_matrix_lr[2, 2] / sum(conf_matrix_lr[, 2])
cat("Precision:", precision_lasso, "\n")

f1_score_lasso <- 2 * (precision_lasso * recall_lasso) / (precision_lasso + recall_lasso)
cat("F1-Score:", f1_score_lasso, "\n")

roc_curve_lasso <- roc(test_y, as.integer(pred_class))
roc_auc_lasso <- auc(roc_curve_lasso)
cat("ROC-AUC Score:", roc_auc_lasso, "\n")

```

### Support Vector Machine model
```{r}
svm_model <- svm(x = train_x, y = train_y, kernel = "radial", cost = 0.1)
predictions_svm <- predict(svm_model, newdata = train_x)
predicted.classes.svm <- ifelse(predictions_svm >=0.5,1,0)
conf_matrix_svm <- table(Actual = train_y, Predicted = predicted.classes.svm)
conf_matrix_svm
```


```{r}
accuracy_svm <- sum(diag(conf_matrix_svm)) / sum(conf_matrix_svm)
cat("Accuracy:", accuracy_svm, "\n")

recall_svm <- conf_matrix_svm[2, 2] / sum(conf_matrix_svm[2, ])
cat("Recall:", recall_svm, "\n")

```


```{r}
#### test
predictions_svm_test <- predict(svm_model, newdata = test_x)
predicted.classes.svm_test <- ifelse(predictions_svm_test >=0.5,1,0)
conf_matrix_svm_test <- table(Actual = test_y, Predicted = predicted.classes.svm_test)
conf_matrix_svm_test
```


```{r}
accuracy_svm <- sum(diag(conf_matrix_svm_test)) / sum(conf_matrix_svm_test)
cat("Accuracy:", accuracy_svm, "\n")

recall_svm <- conf_matrix_svm_test[2, 2] / sum(conf_matrix_svm_test[2, ])
cat("Recall:", recall_svm, "\n")

precision_svm <- conf_matrix_svm_test[2, 2] / sum(conf_matrix_svm_test[, 2])
cat("Precision:", precision_svm, "\n")

f1_score_svm <- 2 * (precision_svm * recall_svm) / (precision_svm + recall_svm)
cat("F1-Score:", f1_score_svm, "\n")

roc_curve_svm <- roc(test_y, as.integer(predicted.classes.svm_test))
roc_auc_svm <- auc(roc_curve_svm)
cat("ROC-AUC Score:", roc_auc_svm, "\n")
```


```{r}
#ROC AUC Score
probabilities <- predict(svm_model, test_x, probability = TRUE)

roc_curve <- roc(test_y, probabilities)
svm_roc_auc <- auc(roc_curve)
print(svm_roc_auc)
```


```{r}
# Plot
plot(roc_curve, main = "ROC AUC Curve for SVM", col = "blue")
# Add AUC score to the plot
legend("bottomright", legend = paste("AUC =", round(auc(roc_curve), 2)), col = "blue", lty = 1, cex = 0.8)

```
### Random Forest Classifier

```{r}
if (is.factor(train_y)) {
  train_y <- as.numeric(train_y)
}
```


```{r}
rf_model <- randomForest(x = train_x, y = train_y, ntree = 100, importance=TRUE, probabilities=TRUE)
```


```{r}
# mtry <- tuneRF(train_x,train_y, ntreeTry=500,
#                stepFactor=1.5,improve=0.01, trace=TRUE, plot=TRUE)
# best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]
# print(mtry)
# print(best.m)
```


```{r}
predictions_train <- predict(rf_model,train_x)
predicted.classes.rf <- ifelse(predictions_train >=0.5,1,0)
confusion_matrix_rf_tt <- table(Actual = train_y, Predicted = predicted.classes.rf)
confusion_matrix_rf_tt
```


```{r}
predictions_test <- predict(rf_model, test_x)
predicted.classes.rf_test <- ifelse(predictions_test >=0.5,1,0)
confusion_matrix_rf_test <- table(Actual = test_y, Predicted = predicted.classes.rf_test)
confusion_matrix_rf_test
```

```{r}
accuracy_rf <- sum(diag(confusion_matrix_rf_test)) / sum(confusion_matrix_rf_test)
cat("Accuracy:", accuracy_rf, "\n")

recall_rf <- confusion_matrix_rf_test[2, 2] / sum(confusion_matrix_rf_test[2, ])
cat("Recall:", recall_rf, "\n")

precision_rf <- confusion_matrix_rf_test[2, 2] / sum(confusion_matrix_rf_test[, 2])
cat("Precision:", precision_rf, "\n")

f1_score_rf <- 2 * (precision_rf * recall_rf) / (precision_rf + recall_rf)
cat("F1-Score:", f1_score_rf, "\n")

roc_curve_rf <- roc(test_y, as.integer(predicted.classes.rf_test))
roc_auc_rf <- auc(roc_curve_rf)
cat("ROC-AUC Score:", roc_auc_rf, "\n")
```

```{r}
#ROC AUC Score
#probabilities_rf <- predict(rf_model, test_x, probability = TRUE)

rf_roc_curve <- roc(test_y, predicted.classes.rf_test)
rf_roc_auc <- auc(rf_roc_curve)
print(rf_roc_auc)

```

```{r}
# Plot
plot(rf_roc_curve, main = "ROC AUC Curve for RF", col = "blue")
# Add AUC score to the plot
legend("bottomright", legend = paste("AUC =", round(auc(rf_roc_curve), 2)), col = "blue", lty = 1, cex = 0.8)
```


```{r}
## Feature importance
feat_imp_df <- importance(rf_model) %>% 
  data.frame() %>% 
  mutate(feature = row.names(.)) 
```


```{r}
ggplot(feat_imp_df, aes(x = reorder(feature, IncNodePurity), 
                        y = IncNodePurity)) +
  geom_bar(stat='identity') +
  coord_flip() +
  theme_classic() +
  labs(
    x     = "Feature",
    y     = "Importance",
    title = "Feature Importance: Random Forest Classifier"
  )

```

### Decile mapping
```{r}
mapped_test_x <- data.frame(test_x, 
                            PredictedProb = 1- predictions_test,
                            prediction = predicted.classes.rf_test,
                            actual_target= test_y)
dim(mapped_test_x)
str(mapped_test_x)

mapped_test_x$decile <- as.numeric(cut_number(mapped_test_x$PredictedProb,10))
unique(mapped_test_x$decile)
table(mapped_test_x$decile)
```


```{r}
#write.csv(mapped_test_x, "decile_output2.csv", row.names=TRUE)
```




```{r}
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
